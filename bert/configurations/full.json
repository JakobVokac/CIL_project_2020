{
  "test_file": "test_preprocessed.txt",
  "bert_model": "bert-large-uncased-whole-word-masking",
  "batch_size": 128,
  "hidden_dim": 256,
  "output_dim": 1,
  "num_layers": 2,
  "bidirectional": true,
  "dropout": 0.25,
  "epochs": 5,
  "train_file": "proc/train_proc_2500000.json",
  "number_tweets": 2500000
}


