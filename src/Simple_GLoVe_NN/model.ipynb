{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595581994490",
   "display_name": "Python 3.7.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.datasets import TextClassificationDataset\n",
    "from torchtext.data import Iterator\n",
    "from torchtext.vocab import Vocab\n",
    "from collections import Counter\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "# regex library\n",
    "import re\n",
    "\n",
    "\n",
    "class TweetProcessor(object):\n",
    "    \"\"\"\n",
    "    pre-process and clean the tweets (works for training and test data)\n",
    "    source: https://towardsdatascience.com/basic-tweet-preprocessing-in-python-efd8360d529ehttps://towardsdatascience.com/basic-tweet-preprocessing-in-python-efd8360d529e\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, source, dest, source2=None):\n",
    "        \"\"\"\n",
    "        case 1: source = train_pos, source2= train_neg\n",
    "        case 2: source = test_data\n",
    "        \"\"\"\n",
    "        self.stopwords = [str(line).replace(\"\\n\", \"\") for line in open(\"../data/stopwords.txt\").readlines()]\n",
    "        self.dictionary = json.load(open(\"../data/data.json\"))\n",
    "        self.source = open(source, \"r\")\n",
    "        if source2 is not None:\n",
    "            self.neg = open(source2, \"r\")\n",
    "        else:\n",
    "            self.neg = None\n",
    "        self.dest = open(dest, \"w+\")\n",
    "\n",
    "    def handle_emojis(self, tweet):\n",
    "        for positive in self.dictionary[\"POS_EMOJI\"]:\n",
    "            regex = positive[\"regex\"]\n",
    "            tweet = re.sub(regex, positive[\"replacement\"], tweet)\n",
    "        for negative in self.dictionary[\"NEG_EMOJI\"]:\n",
    "            regex = negative[\"regex\"]\n",
    "            tweet = re.sub(regex, negative[\"replacement\"], tweet)\n",
    "        return tweet\n",
    "\n",
    "    def preprocess_word(self, word):\n",
    "        # remove punctuation\n",
    "        word = word.strip('\\'\"?!,.():;')\n",
    "        for entry in self.dictionary[\"WORD_CLEANING\"]:\n",
    "            word = re.sub(entry[\"regex\"], entry[\"replacement\"], word)\n",
    "        return word\n",
    "\n",
    "    def preprocess_tweet(self, tweet):\n",
    "        for entry in self.dictionary[\"TWEET_CLEANING\"]:\n",
    "            tweet = re.sub(entry[\"regex\"], entry[\"replacement\"], tweet)\n",
    "        return tweet\n",
    "\n",
    "    def is_valid_word(self, word):\n",
    "        for validity in self.dictionary[\"WORD_VALIDITY\"]:\n",
    "            if re.search(validity[\"regex\"], word) is not None:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def clean(self, tweet):\n",
    "        processed_tweet = []\n",
    "        # preprocess tweet as a whole\n",
    "        tweet = self.preprocess_tweet(tweet)\n",
    "        # strip space\n",
    "        tweet = tweet.strip(' \"\\'')\n",
    "        # replace emojis with either POS_EMOJI or NEG_EMOJI\n",
    "        tweet = self.handle_emojis(tweet)\n",
    "        words = tweet.split()\n",
    "        for word in words:\n",
    "            # pre-process word\n",
    "            word = self.preprocess_word(word)\n",
    "            if self.is_valid_word(word) and word not in self.stopwords:\n",
    "                processed_tweet.append(word)\n",
    "        # return tweet as string\n",
    "        return ' '.join(processed_tweet)\n",
    "\n",
    "    def preprocess(self):\n",
    "        # distinguish case\n",
    "        if self.neg is None:\n",
    "            # Test\n",
    "            files = [self.source]\n",
    "        else:\n",
    "            # Train\n",
    "            files = [self.source, self.neg]\n",
    "        tweet_id = 1\n",
    "        for file in files:\n",
    "            for tweet in file.readlines():\n",
    "                tweet = self.clean(tweet)\n",
    "\n",
    "                # write to dest\n",
    "                if self.neg is not None:  # write labels\n",
    "                    if file is self.source:  # label pos = 1\n",
    "                        self.dest.write(str(tweet_id) + ',' + tweet.replace('\\n', '') + ',' + str(1) + '\\n')\n",
    "                    else:  # label neg = 0\n",
    "                        self.dest.write(str(tweet_id) + ',' + tweet.replace('\\n', '') + ',' + str(0) + '\\n')\n",
    "                else:  # do not write labels\n",
    "                    self.dest.write(str(tweet_id) + ',' + tweet.replace('\\n', '') + '\\n')\n",
    "                # increment id\n",
    "                tweet_id += 1\n",
    "            # close file\n",
    "            file.close()\n",
    "        # close dest file\n",
    "        print('\\nSaved processed tweets to: %s' % str(self.dest.name))\n",
    "        self.dest.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_pos_full = \"../data/train_pos.txt\"\n",
    "train_neg_full = \"../data/train_neg.txt\"\n",
    "train_processed_file_name = \"../data/train_preprocessed.txt\"\n",
    "train_cleaned = TweetProcessor(train_pos_full, train_processed_file_name, train_neg_full)\n",
    "train_cleaned.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"../data/train_preprocessed.txt\") as f:\n",
    "    data_list = f.readlines()\n",
    "\n",
    "data_list = [x.strip().split(',')[1:] for x in data_list]\n",
    "\n",
    "data_pos = [x[0] for x in data_list if x[1] == '1']\n",
    "data_neg = [x[0] for x in data_list if x[1] == '1']\n",
    "\n",
    "data_vocab = [x[0].split() for x in data_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "vocab = Vocab(Counter(list(chain.from_iterable(data_vocab))))\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "temp = data_vocab[0][0]\n",
    "bigram_list = []\n",
    "\n",
    "for sent in data_vocab:\n",
    "    for word in sent:\n",
    "        bigram_list.append(temp + ' ' + word)\n",
    "        temp = word\n",
    "\n",
    "bigram_vocab = Vocab(Counter(bigram_list[1:]))\n",
    "bigram_data = []\n",
    "for inst in data_list:\n",
    "     sent = inst[0].split()\n",
    "     sent = [x + ' ' + y for x,y in zip(sent[:-1],sent[1:])]\n",
    "     label = int(inst[1])\n",
    "     tokens = [bigram_vocab.stoi[x] for x in sent]\n",
    "     bigram_data.append((label,tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(bigram_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_data = []\n",
    "for inst in data_list:\n",
    "     sent = inst[0].split()\n",
    "     label = int(inst[1])\n",
    "     tokens = [vocab.stoi[x] for x in sent]\n",
    "     dataset_data.append((label,tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(dataset_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = {1,0}\n",
    "size = len(dataset_data)\n",
    "split = int(size*0.90)\n",
    "print(size,split)\n",
    "train_data = dataset_data[:split]\n",
    "valid_data = dataset_data[split:]\n",
    "print(len(train_data),len(valid_data))\n",
    "random.shuffle(train_data)\n",
    "train_dataset = TextClassificationDataset(vocab,train_data,labels)\n",
    "valid_dataset = TextClassificationDataset(vocab,valid_data,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {1,0}\n",
    "size = len(bigram_data)\n",
    "split = int(size*0.90)\n",
    "print(size,split)\n",
    "bigram_train_data = bigram_data[:split]\n",
    "bigram_valid_data = bigram_data[split:]\n",
    "print(len(bigram_train_data),len(bigram_valid_data))\n",
    "random.shuffle(bigram_train_data)\n",
    "bigram_train = TextClassificationDataset(bigram_vocab,bigram_train_data,labels)\n",
    "bigram_valid = TextClassificationDataset(bigram_vocab,bigram_valid_data,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(x):\n",
    "    if x > .5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def get_pred_arr(arr):\n",
    "    return [get_pred(x) for x in arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "dim1 = 80\n",
    "dim2 = 40\n",
    "dim3 = 20\n",
    "dropout = 0.3\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets = None):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.sig(self.fc(embedded))\n",
    "\n",
    "model = Classifier(vocab_size,dim1,1)\n",
    "loss_func = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 10\n",
    "num_batches = len(train_dataset)//batch_size\n",
    "\n",
    "\n",
    "def generate_batch(batch):\n",
    "\n",
    "    label = torch.tensor([entry[0] for entry in batch if len(entry[1]) != 0],dtype=torch.float)\n",
    "    text = [torch.tensor(entry[1]) for entry in batch if len(entry[1]) != 0]\n",
    "\n",
    "    offsets = [0] + [len(entry) for entry in text]\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "\n",
    "    if text == []:\n",
    "        return None, None, None\n",
    "    text = torch.cat(text)\n",
    "    return text, offsets, label\n",
    "\n",
    "def train_model(model, loss_func, optimizer, train_dataset, valid_dataset, batch_size = 10, epochs = 10):\n",
    "    num_batches = len(train_dataset)//batch_size\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for i in range(num_batches-1):\n",
    "            \n",
    "            \n",
    "            text, offsets, labels = generate_batch(train_dataset[i*batch_size:(i+1)*batch_size])\n",
    "            if text == None:\n",
    "                continue\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(text,offsets)\n",
    "\n",
    "            loss = loss_func(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss = loss.item()\n",
    "            if i % 1000 == 0:\n",
    "                print(\"Epoch: \", epoch, \" batch: \", i, \" num_batches: \", num_batches, \" loss: \", loss.item())\n",
    "        \n",
    "\n",
    "        text, offsets, labels = generate_batch(valid_dataset)\n",
    "\n",
    "        outputs = model(text,offsets)\n",
    "        np_outs = outputs.detach().numpy()\n",
    "        preds = get_pred_arr(np_outs)\n",
    "\n",
    "        precision = sum([x == y for x,y in zip(preds,labels.detach().numpy())])/len(preds)       \n",
    "\n",
    "        print(\"Epoch: %d, loss = %.3f, precision = %.3f\" % (epoch, epoch_loss, precision))\n",
    "\n",
    "    return model\n",
    "\n",
    "train_model(model, loss_func, optimizer, train_dataset, valid_dataset)\n",
    "\n",
    "model2 = Classifier(bigram_vocab_size,dim1,1)\n",
    "\n",
    "train_model(model2, loss_func, optimizer, bigram_train, bigram_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}